# AI Mastery Series - Machine Learning Fundamentals

## Overview

This repository contains three comprehensive Jupyter notebooks that explore the fundamental concepts of machine learning and natural language processing. The notebooks progress from basic mathematical principles to advanced tokenization techniques used in modern AI systems.

## Notebooks

### 1. AIMastery-Session01.ipynb
**Topic**: Mathematics Behind Machine Learning with Visualizations

**Description**: This notebook provides a comprehensive introduction to the mathematical foundations of machine learning, featuring interactive visualizations and step-by-step explanations of how machines learn.

**Key Topics Covered**:
- **Linear Regression**: Understanding the foundation of ML with the equation y = mx + c
- **Cost Functions**: Sum of Squared Errors (SSE) and Mean Squared Error (MSE)
- **Gradient Descent**: The optimization algorithm that enables machine learning
- **Learning Rate**: How to control the step size in optimization
- **3D Visualizations**: Interactive plots showing error surfaces and optimization paths
- **Animated Learning**: GIF generation showing how the algorithm converges
- **Advanced Examples**: Neural networks, SVMs, and activation functions

**Highlights**:
- Complete installation guide for beginners
- Step-by-step gradient descent implementation
- Beautiful 3D visualizations of the optimization landscape
- Real-world applications including handwritten digit recognition
- Mathematical foundations using SymPy for symbolic computation

**Learning Outcomes**:
- Understand how machines learn through iterative optimization
- Visualize the relationship between parameters and error
- Grasp the mathematical principles behind modern AI systems

### 2. WordsVsTokens-Embeddings.ipynb
**Topic**: Why Tokens Are Used Over Words (Detailed Explanation)

**Description**: This notebook explains the fundamental concept of tokenization in natural language processing and why modern AI systems break words into smaller units called tokens.

**Key Topics Covered**:
- **The Million-Word Problem**: Why word-level embeddings are inefficient
- **Tokenization Benefits**: Memory efficiency, better generalization, handling unknown words
- **Subword Reusability**: How tokens like "un-", "break", and "-able" appear across multiple words
- **Semantic Clustering**: How similar tokens naturally group together
- **Vocabulary Compression**: Reducing millions of words to ~30K tokens
- **Interactive Visualizations**: 3D plots showing token relationships in embedding space

**Highlights**:
- Clear explanation of why transformers use tokenization
- Practical examples showing token reuse (e.g., "unbreakable" â†’ ["un", "break", "able"])
- Simple tokenizer implementation without requiring heavy libraries
- Beautiful 3D visualizations of token clustering
- Real-world efficiency comparisons

**Learning Outcomes**:
- Understand why modern AI doesn't use whole words
- Learn how tokenization enables efficient language processing
- Visualize how tokens cluster in semantic space

### 3. WordsTokensUsingPytorch.ipynb
**Topic**: PyTorch Implementation of Tokenization

**Description**: This notebook demonstrates the same tokenization concepts as the previous notebook but implemented using PyTorch and the Transformers library, showing real-world tokenization in action.

**Key Topics Covered**:
- **BERT Tokenizer**: Using pre-trained tokenizers from Hugging Face
- **Real Token Embeddings**: Extracting actual embeddings from BERT
- **3D Visualization**: Interactive plots using real transformer embeddings
- **Dimensionality Reduction**: PCA and t-SNE for visualizing high-dimensional embeddings
- **Token Analysis**: Frequency analysis and categorization
- **Practical Implementation**: Working with real NLP models

**Highlights**:
- Hands-on experience with PyTorch and Transformers
- Real BERT tokenization and embeddings
- Professional-grade visualization with Plotly
- Comprehensive error handling and fallback options
- Production-ready code examples

**Learning Outcomes**:
- Practical experience with modern NLP libraries
- Understanding of real transformer tokenization
- Skills for implementing tokenization in projects

## Getting Started

### Prerequisites
- Python 3.8 or higher
- Jupyter Notebook or JupyterLab
- Virtual environment (recommended)

### Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd AIMastery
   ```

2. **Create a virtual environment**:
   ```bash
   python -m venv aimastery_env
   source aimastery_env/bin/activate  # On Windows: aimastery_env\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   # For AIMastery-Session01.ipynb
   pip install pandas numpy matplotlib seaborn sympy scikit-learn statsmodels imageio
   
   # For WordsVsTokens-Embeddings.ipynb
   pip install plotly pandas numpy scikit-learn
   
   # For WordsTokensUsingPytorch.ipynb
   pip install torch transformers plotly pandas scikit-learn matplotlib nltk
   ```

4. **Start Jupyter Notebook**:
   ```bash
   jupyter notebook
   ```

### Recommended Learning Path

1. **Start with AIMastery-Session01.ipynb** - Learn the mathematical foundations
2. **Move to WordsVsTokens-Embeddings.ipynb** - Understand tokenization concepts
3. **Complete with WordsTokensUsingPytorch.ipynb** - See real-world implementation

## Key Concepts Explained

### Why This Series Matters

Modern AI systems like GPT, BERT, and other transformers all rely on the fundamental concepts covered in these notebooks:

1. **Mathematical Optimization**: Every AI model uses gradient descent or similar optimization techniques
2. **Tokenization**: All language models break text into tokens for efficiency
3. **Embeddings**: Vector representations are the foundation of modern NLP

### From Simple to Complex

The progression shows how:
- **Simple linear regression** scales to **complex neural networks**
- **Basic tokenization** enables **powerful language models**
- **Mathematical principles** remain constant across all AI systems

### Practical Applications

These concepts are used in:
- **Language Models**: GPT, BERT, LLaMA
- **Machine Translation**: Google Translate, DeepL
- **Content Generation**: ChatGPT, Claude, Gemini
- **Search Engines**: Semantic search, question answering
- **Recommendation Systems**: Content personalization

## Visualizations

Each notebook includes rich visualizations:
- **3D optimization landscapes** showing gradient descent paths
- **Interactive token clustering** in embedding space
- **Animated learning processes** showing convergence
- **Error surface visualizations** for understanding optimization

## Technical Details

### AIMastery-Session01.ipynb
- **Language**: Python with NumPy, Pandas, Matplotlib
- **Concepts**: Linear algebra, calculus, optimization
- **Visualizations**: 3D plots, animations, contour maps
- **Difficulty**: Beginner to Intermediate

### WordsVsTokens-Embeddings.ipynb
- **Language**: Python with Scikit-learn, Plotly
- **Concepts**: NLP, tokenization, embeddings
- **Visualizations**: 3D scatter plots, interactive charts
- **Difficulty**: Intermediate

### WordsTokensUsingPytorch.ipynb
- **Language**: Python with PyTorch, Transformers
- **Concepts**: Deep learning, BERT, real tokenization
- **Visualizations**: Professional interactive plots
- **Difficulty**: Intermediate to Advanced

## Contributing

This is an educational repository. If you find errors or have suggestions for improvements, please open an issue or submit a pull request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- **Hugging Face**: For the Transformers library and pre-trained models
- **PyTorch**: For the deep learning framework
- **Plotly**: For interactive visualizations
- **Scikit-learn**: For machine learning utilities
- **NumPy/Pandas**: For numerical computing and data manipulation

## Future Sessions

This series will continue with:
- **Session 2**: Generative AI and LLMs
- **Session 3**: Advanced techniques with RAG and Model Context Protocol
- **Session 4**: Building no-code AI agents

---

**Happy Learning!** ðŸš€

*These notebooks provide a solid foundation for understanding modern AI systems. The mathematical principles you learn here are the same ones powering the most advanced AI models today.*