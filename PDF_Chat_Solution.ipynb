{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Chat Solution with LLM\n",
    "\n",
    "A simple and lightweight solution to chat with PDF documents using LLM (Large Language Models).\n",
    "\n",
    "## Features\n",
    "- Extract text and images from PDF files\n",
    "- Summarize PDF content using LLM\n",
    "- Ask questions about the PDF content\n",
    "- Support for both Mistral and OpenAI models\n",
    "- Save extracted images as JPG files\n",
    "- Self-contained with all required PDF processing functions included\n",
    "\n",
    "## Requirements\n",
    "Make sure you have the following packages installed:\n",
    "```bash\n",
    "pip install pdfplumber pymupdf pillow mistralai openai\n",
    "```\n",
    "\n",
    "## What's Included\n",
    "- **PDF Text Extraction**: Extract text, tables, and hyperlinks from PDF files\n",
    "- **PDF Image Extraction**: Extract and save images from PDF files as JPG\n",
    "- **LLM Integration**: Chat with your PDF using Mistral or OpenAI models\n",
    "- **Streaming Support**: Real-time text generation responses\n",
    "- **No External Dependencies**: All PDF processing functions are included in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (0.11.7)\n",
      "Requirement already satisfied: pymupdf in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (1.26.3)\n",
      "Requirement already satisfied: pillow in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (11.2.1)\n",
      "Requirement already satisfied: mistralai in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (1.9.2)\n",
      "Requirement already satisfied: openai in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (1.93.0)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from pdfminer.six==20250506->pdfplumber) (45.0.5)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from mistralai) (0.2.2)\n",
      "Requirement already satisfied: httpx>=0.28.1 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from mistralai) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from mistralai) (2.11.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from mistralai) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from mistralai) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from httpx>=0.28.1->mistralai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /Users/satishkumar/pythonvirtualenvs/langkg/lib/python3.9/site-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "!pip install pdfplumber pymupdf pillow mistralai openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Image Extraction Function\n",
    "\n",
    "This function extracts images from PDF files and saves them as JPG files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path: str, output_dir: str = None, debug: bool = False):\n",
    "    \"\"\"\n",
    "    Extract images from PDF and save them as JPG files with proper colorspace handling.\n",
    "    \n",
    "    This enhanced version properly handles CMYK colorspace conversion and provides\n",
    "    debugging information to identify why images might appear blank.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        output_dir (str): Directory to save extracted images (default: same as PDF)\n",
    "        debug (bool): Enable debug output to troubleshoot image extraction issues\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of dictionaries containing image information with keys:\n",
    "                   - page_number: Page number where image was found\n",
    "                   - image_index: Index of image on that page\n",
    "                   - filename: Name of the saved image file\n",
    "                   - path: Full path to the saved image\n",
    "                   - width: Image width in pixels\n",
    "                   - height: Image height in pixels\n",
    "                   - size_bytes: Size of image data in bytes\n",
    "                   - colorspace: Original colorspace of the image\n",
    "                   - converted: Whether colorspace conversion was applied\n",
    "    \"\"\"\n",
    "    import fitz  # PyMuPDF\n",
    "    import os\n",
    "    from typing import List, Dict, Any\n",
    "    import io\n",
    "    from PIL import Image\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(pdf_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Open PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    image_info = []\n",
    "    image_count = 0\n",
    "    \n",
    "    # Base name for output files\n",
    "    base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"üîç Starting image extraction from: {pdf_path}\")\n",
    "        print(f\"üìÑ Total pages: {len(doc)}\")\n",
    "    \n",
    "    # Iterate through pages\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        if debug and image_list:\n",
    "            print(f\"üìÑ Page {page_num + 1}: Found {len(image_list)} images\")\n",
    "        \n",
    "        # Extract images from this page\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            \n",
    "            try:\n",
    "                # Method 1: Try direct pixmap extraction (recommended)\n",
    "                pix = fitz.Pixmap(doc, xref)\n",
    "                \n",
    "                if debug:\n",
    "                    colorspace_name = pix.colorspace.name if pix.colorspace else \"Unknown\"\n",
    "                    print(f\"  üñºÔ∏è  Image {img_index + 1}: {pix.width}x{pix.height}, \"\n",
    "                          f\"colorspace: {colorspace_name}, channels: {pix.n}\")\n",
    "                \n",
    "                # Skip if image is too small\n",
    "                if pix.width < 50 or pix.height < 50:\n",
    "                    if debug:\n",
    "                        print(f\"    ‚ö†Ô∏è  Skipping small image: {pix.width}x{pix.height}\")\n",
    "                    pix = None\n",
    "                    continue\n",
    "                \n",
    "                converted = False\n",
    "                original_colorspace = pix.colorspace.name if pix.colorspace else \"Unknown\"\n",
    "                \n",
    "                # Enhanced colorspace handling\n",
    "                if pix.colorspace is None:\n",
    "                    if debug:\n",
    "                        print(f\"    ‚ö†Ô∏è  No colorspace information, trying alternative method\")\n",
    "                    # Try alternative extraction method\n",
    "                    pix = None\n",
    "                    img_dict = doc.extract_image(xref)\n",
    "                    img_data = img_dict[\"image\"]\n",
    "                    \n",
    "                    # Save using PIL directly\n",
    "                    image_filename = f\"{base_name}_page{page_num + 1}_img{img_index + 1}.jpg\"\n",
    "                    image_path = os.path.join(output_dir, image_filename)\n",
    "                    \n",
    "                    with open(image_path, \"wb\") as img_file:\n",
    "                        img_file.write(img_data)\n",
    "                    \n",
    "                    # Try to get dimensions from PIL\n",
    "                    try:\n",
    "                        with Image.open(io.BytesIO(img_data)) as pil_img:\n",
    "                            width, height = pil_img.size\n",
    "                    except:\n",
    "                        width, height = 0, 0\n",
    "                    \n",
    "                    image_info.append({\n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"image_index\": img_index + 1,\n",
    "                        \"filename\": image_filename,\n",
    "                        \"path\": image_path,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height,\n",
    "                        \"size_bytes\": len(img_data),\n",
    "                        \"colorspace\": \"Unknown\",\n",
    "                        \"converted\": False,\n",
    "                        \"extraction_method\": \"direct_bytes\"\n",
    "                    })\n",
    "                    \n",
    "                    image_count += 1\n",
    "                    if debug:\n",
    "                        print(f\"    ‚úÖ Saved using direct bytes method\")\n",
    "                    continue\n",
    "                \n",
    "                # Handle different colorspaces\n",
    "                if pix.colorspace and pix.colorspace.name in [\"DeviceCMYK\", \"CMYK\"]:\n",
    "                    if debug:\n",
    "                        print(f\"    üîÑ Converting CMYK to RGB\")\n",
    "                    # Convert CMYK to RGB\n",
    "                    rgb_pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                    pix = rgb_pix\n",
    "                    converted = True\n",
    "                    \n",
    "                elif pix.n - pix.alpha > 4:  # More than 4 channels (likely CMYK with alpha)\n",
    "                    if debug:\n",
    "                        print(f\"    üîÑ Converting multi-channel image to RGB\")\n",
    "                    rgb_pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                    pix = rgb_pix\n",
    "                    converted = True\n",
    "                \n",
    "                elif pix.n - pix.alpha == 4:  # Exactly 4 channels, likely CMYK\n",
    "                    if debug:\n",
    "                        print(f\"    üîÑ Converting 4-channel image to RGB\")\n",
    "                    rgb_pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                    pix = rgb_pix\n",
    "                    converted = True\n",
    "                \n",
    "                # Save the image\n",
    "                image_filename = f\"{base_name}_page{page_num + 1}_img{img_index + 1}.jpg\"\n",
    "                image_path = os.path.join(output_dir, image_filename)\n",
    "                \n",
    "                # Save as PNG first to preserve quality, then convert to JPG\n",
    "                png_data = pix.tobytes(\"png\")\n",
    "                \n",
    "                # Use PIL for final JPG conversion to ensure quality\n",
    "                pil_image = Image.open(io.BytesIO(png_data))\n",
    "                \n",
    "                # Ensure RGB mode for JPG\n",
    "                if pil_image.mode in (\"RGBA\", \"LA\", \"P\"):\n",
    "                    pil_image = pil_image.convert(\"RGB\")\n",
    "                elif pil_image.mode not in (\"RGB\", \"L\"):\n",
    "                    pil_image = pil_image.convert(\"RGB\")\n",
    "                \n",
    "                # Save as high-quality JPG\n",
    "                pil_image.save(image_path, \"JPEG\", quality=95, optimize=True)\n",
    "                \n",
    "                # Store image information\n",
    "                image_info.append({\n",
    "                    \"page_number\": page_num + 1,\n",
    "                    \"image_index\": img_index + 1,\n",
    "                    \"filename\": image_filename,\n",
    "                    \"path\": image_path,\n",
    "                    \"width\": pix.width,\n",
    "                    \"height\": pix.height,\n",
    "                    \"size_bytes\": len(png_data),\n",
    "                    \"colorspace\": original_colorspace,\n",
    "                    \"converted\": converted,\n",
    "                    \"extraction_method\": \"pixmap\"\n",
    "                })\n",
    "                \n",
    "                image_count += 1\n",
    "                \n",
    "                if debug:\n",
    "                    conversion_msg = \" (converted)\" if converted else \"\"\n",
    "                    print(f\"    ‚úÖ Saved: {image_filename}{conversion_msg}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"    ‚ùå Error extracting image {img_index + 1}: {e}\")\n",
    "                    print(f\"    üîÑ Trying alternative extraction method...\")\n",
    "                \n",
    "                # Alternative extraction method\n",
    "                try:\n",
    "                    img_dict = doc.extract_image(xref)\n",
    "                    img_data = img_dict[\"image\"]\n",
    "                    \n",
    "                    image_filename = f\"{base_name}_page{page_num + 1}_img{img_index + 1}.jpg\"\n",
    "                    image_path = os.path.join(output_dir, image_filename)\n",
    "                    \n",
    "                    # Save directly\n",
    "                    with open(image_path, \"wb\") as img_file:\n",
    "                        img_file.write(img_data)\n",
    "                    \n",
    "                    # Try to get dimensions\n",
    "                    try:\n",
    "                        with Image.open(io.BytesIO(img_data)) as pil_img:\n",
    "                            width, height = pil_img.size\n",
    "                    except:\n",
    "                        width, height = 0, 0\n",
    "                    \n",
    "                    image_info.append({\n",
    "                        \"page_number\": page_num + 1,\n",
    "                        \"image_index\": img_index + 1,\n",
    "                        \"filename\": image_filename,\n",
    "                        \"path\": image_path,\n",
    "                        \"width\": width,\n",
    "                        \"height\": height,\n",
    "                        \"size_bytes\": len(img_data),\n",
    "                        \"colorspace\": \"Unknown\",\n",
    "                        \"converted\": False,\n",
    "                        \"extraction_method\": \"fallback\"\n",
    "                    })\n",
    "                    \n",
    "                    image_count += 1\n",
    "                    if debug:\n",
    "                        print(f\"    ‚úÖ Saved using fallback method\")\n",
    "                        \n",
    "                except Exception as e2:\n",
    "                    if debug:\n",
    "                        print(f\"    ‚ùå Both methods failed: {e2}\")\n",
    "                    continue\n",
    "            \n",
    "            finally:\n",
    "                # Clean up pixmap to free memory\n",
    "                if 'pix' in locals() and pix:\n",
    "                    pix = None\n",
    "    \n",
    "    # Close document\n",
    "    doc.close()\n",
    "    \n",
    "    success_msg = f\"‚úÖ Successfully extracted {image_count} images from {pdf_path}\"\n",
    "    if debug:\n",
    "        print(f\"\\n{success_msg}\")\n",
    "        if image_info:\n",
    "            print(\"\\nüìä Extraction Summary:\")\n",
    "            for img in image_info:\n",
    "                method = img.get('extraction_method', 'pixmap')\n",
    "                converted = ' (converted)' if img.get('converted', False) else ''\n",
    "                print(f\"  ‚Ä¢ {img['filename']}: {img['width']}x{img['height']}, \"\n",
    "                      f\"{img['colorspace']}{converted} [{method}]\")\n",
    "    else:\n",
    "        print(success_msg)\n",
    "    \n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Text Extraction Function\n",
    "\n",
    "This function extracts text content from PDF files, including tables and hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_to_file(outputfile, text, table_text):\n",
    "    \"\"\"\n",
    "    Write text and table data to a file.\n",
    "    \n",
    "    Args:\n",
    "        outputfile (str): Path to the output text file\n",
    "        text (str): Main text content\n",
    "        table_text (str): Table content in text format\n",
    "        \n",
    "    Returns:\n",
    "        str: Combined text content\n",
    "    \"\"\"\n",
    "    combined_text = text + \"\\n\\n\" + table_text\n",
    "    with open(outputfile, 'w', encoding='utf-8') as text_file:\n",
    "        text_file.write(combined_text)\n",
    "    return combined_text\n",
    "\n",
    "def pdf_to_text(file_path, outputfile, extracttables=True, extractimages=False):\n",
    "    \"\"\"\n",
    "    Extract text from PDF file, including tables and hyperlinks.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file\n",
    "        outputfile (str): Path to save the extracted text\n",
    "        extracttables (bool): Whether to extract tables (default: True)\n",
    "        extractimages (bool): Whether to extract images (default: False)\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    import pdfplumber\n",
    "    \n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        # Extract text from all pages\n",
    "        text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
    "        \n",
    "        # Extract hyperlinks\n",
    "        try:\n",
    "            hyperlinks_text = \"\\n\".join([str(hyperlink) for hyperlink in pdf.hyperlinks])\n",
    "            text = \"\\n\".join([text, \"\\nHyperlinks and Annotations in the document:\", hyperlinks_text])\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract hyperlinks: {e}\")\n",
    "\n",
    "    # Extract tables if requested\n",
    "    table_text = \"\"\n",
    "    if extracttables:\n",
    "        table_text = extract_tables_from_pdf(file_path)\n",
    "    \n",
    "    # Write to file and return content\n",
    "    return write_text_to_file(outputfile, text, table_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Table Extraction Function\n",
    "\n",
    "This function extracts tables from PDF files and formats them as text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing Functions\n",
    "\n",
    "The following cells contain the required functions for PDF text and image extraction. These functions are self-contained and don't require external files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extract table data in text format from PDF file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted table text with all tables found in the PDF\n",
    "    \"\"\"\n",
    "    import pdfplumber\n",
    "    \n",
    "    counter = 1\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        table_text = []\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                table_text.append(\"Table \"+str(counter)+\"\\n\")\n",
    "                for row in table:\n",
    "                    table_text.append(\" | \".join([str(cell) for cell in row]))\n",
    "                table_text.append(\"\\n\")\n",
    "                counter = counter+1\n",
    "    return \"\\n\".join(table_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral available: True\n",
      "OpenAI available: True\n",
      "All required PDF processing functions are now included in the notebook!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Import LLM clients\n",
    "try:\n",
    "    from mistralai import Mistral, UserMessage, SystemMessage, AssistantMessage, ToolMessage\n",
    "    MISTRAL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MISTRAL_AVAILABLE = False\n",
    "    print(\"Mistral client not available. Install with: pip install mistralai\")\n",
    "\n",
    "try:\n",
    "    import openai\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"OpenAI client not available. Install with: pip install openai\")\n",
    "\n",
    "print(f\"Mistral available: {MISTRAL_AVAILABLE}\")\n",
    "print(f\"OpenAI available: {OPENAI_AVAILABLE}\")\n",
    "\n",
    "print(\"All required PDF processing functions are now included in the notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Keys\n",
    "\n",
    "Set your API keys here. You can get free API keys from:\n",
    "- **Mistral**: https://console.mistral.ai/\n",
    "- **OpenAI**: https://platform.openai.com/\n",
    "- Store the openai api key in ~/openai.key file.\n",
    "- Store the mistral api key in ~/mistral.key file.\n",
    "\n",
    "**Note:**\n",
    "- \"~\" is your home directory.\n",
    "- In case of Mac, home directory: /home/<mac_user_name>\n",
    "- In case of Windows, it is C:\\Users\\<windows_user_name>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = \"OPENAI_KEY\"\n",
    "OPENAI_API_KEY_FILE_NAME = \"openai.key\"\n",
    "\n",
    "MISTRAL_KEY = \"MISTRAL_KEY\"\n",
    "MISTRAL_API_KEY_FILE_NAME = \"mistral.key\"\n",
    "\n",
    "ANTHROPIC_CLAUDE_KEY = \"ANTHROPIC_CLAUDE_KEY\"\n",
    "ANTHROPIC_CLAUDE_KEY_FILE_NAME = \"anthropic.key\"\n",
    "\n",
    "GROK_KEY = \"GROK_KEY\"\n",
    "GROK_KEY_FILE_NAME = \"grok.key\"\n",
    "\n",
    "GEMINI_KEY = \"GEMINI_KEY\"\n",
    "GEMINI_KEY_FILE_NAME = \"gemini.key\"\n",
    "\n",
    "def get_api_key(keyName, keyFileName):\n",
    "    key = \"Not Set\"\n",
    "    try:\n",
    "        key = os.environ[keyName]\n",
    "    except:\n",
    "        file_name = os.path.join(os.path.expanduser('~'), keyFileName)\n",
    "        key_file_name = Path(file_name)\n",
    "        if key_file_name.exists():\n",
    "            with open(file_name, \"r\") as keyfile:\n",
    "                key = keyfile.read().strip()\n",
    "        else:\n",
    "            errMessage = \"{keyName} is not found\"\n",
    "            print(errMessage.format(keyName=keyName))\n",
    "    return key\n",
    "\n",
    "def get_openai_api_key():\n",
    "    return get_api_key(OPENAI_KEY, OPENAI_API_KEY_FILE_NAME)\n",
    "\n",
    "def get_mistral_api_key():\n",
    "    return get_api_key(MISTRAL_KEY, MISTRAL_API_KEY_FILE_NAME)\n",
    "\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "MISTRAL_API_KEY = get_mistral_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MISTRAL as LLM provider\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"mistral_api_key\": os.getenv(\"MISTRAL_API_KEY\", MISTRAL_API_KEY),\n",
    "    \"openai_api_key\": os.getenv(\"OPENAI_API_KEY\", OPENAI_API_KEY),\n",
    "    \"model_provider\": \"mistral\",  # or \"openai\"\n",
    "    \"mistral_model\": \"mistral-large-latest\",\n",
    "    \"openai_model\": \"gpt-3.5-turbo\",\n",
    "    \"max_tokens\": 4000\n",
    "}\n",
    "\n",
    "# You can change the model provider here\n",
    "# CONFIG[\"model_provider\"] = \"openai\"  # Switch to OpenAI\n",
    "\n",
    "print(f\"Using {CONFIG['model_provider'].upper()} as LLM provider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Chat Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ PDF Chat Bot initialized!\n"
     ]
    }
   ],
   "source": [
    "class PDFChatBot:\n",
    "    \"\"\"Simple PDF Chat Bot with LLM integration\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.pdf_content = None\n",
    "        self.pdf_path = None\n",
    "        self.extracted_images = []\n",
    "        \n",
    "        # Initialize LLM client\n",
    "        if config[\"model_provider\"] == \"mistral\":\n",
    "            if not MISTRAL_AVAILABLE:\n",
    "                raise ImportError(\"Mistral client not available\")\n",
    "            self.client = Mistral(api_key=config[\"mistral_api_key\"])\n",
    "        elif config[\"model_provider\"] == \"openai\":\n",
    "            if not OPENAI_AVAILABLE:\n",
    "                raise ImportError(\"OpenAI client not available\")\n",
    "            openai.api_key = config[\"openai_api_key\"]\n",
    "            self.client = openai\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model provider: {config['model_provider']}\")\n",
    "    \n",
    "    def load_pdf(self, pdf_path: str, extract_images: bool = True, debug: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Load and process PDF file\"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        self.pdf_path = pdf_path\n",
    "        print(f\"Loading PDF: {pdf_path}\")\n",
    "        \n",
    "        # Extract text\n",
    "        output_dir = os.path.dirname(pdf_path) or \".\"\n",
    "        text_output = os.path.join(output_dir, os.path.splitext(os.path.basename(pdf_path))[0] + \".txt\")\n",
    "        \n",
    "        try:\n",
    "            # Extract text using existing extractor\n",
    "            text_content = pdf_to_text(pdf_path, text_output)\n",
    "            \n",
    "            # Read the extracted text\n",
    "            with open(text_output, 'r', encoding='utf-8') as f:\n",
    "                self.pdf_content = f.read()\n",
    "            \n",
    "            # Extract images if requested\n",
    "            if extract_images:\n",
    "                try:\n",
    "                    self.extracted_images = extract_images_from_pdf(pdf_path, output_dir, debug=debug)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not extract images: {e}\")\n",
    "                    self.extracted_images = []\n",
    "            \n",
    "            result = {\n",
    "                \"text_length\": len(self.pdf_content),\n",
    "                \"text_file\": text_output,\n",
    "                \"images_extracted\": len(self.extracted_images),\n",
    "                \"images\": self.extracted_images\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ PDF loaded successfully!\")\n",
    "            print(f\"üìÑ Text length: {result['text_length']} characters\")\n",
    "            print(f\"üñºÔ∏è Images extracted: {result['images_extracted']}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading PDF: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _call_mistral(self, messages: List) -> str:\n",
    "        \"\"\"Call Mistral API with new client and method\"\"\"\n",
    "        response = self.client.chat.complete(\n",
    "            model=self.config[\"mistral_model\"],\n",
    "            messages=messages,\n",
    "            max_tokens=self.config[\"max_tokens\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def _call_mistral_stream(self, messages: List):\n",
    "        \"\"\"Call Mistral API with streaming and new client\"\"\"\n",
    "        stream = self.client.chat.stream(\n",
    "            model=self.config[\"mistral_model\"],\n",
    "            messages=messages,\n",
    "            max_tokens=self.config[\"max_tokens\"]\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.data.choices[0].delta.content is not None:\n",
    "                yield chunk.data.choices[0].delta.content\n",
    "    \n",
    "    def _call_openai(self, messages: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Call OpenAI API\"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.config[\"openai_model\"],\n",
    "            messages=messages,\n",
    "            max_tokens=self.config[\"max_tokens\"]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def summarize(self, max_length: int = 1000) -> str:\n",
    "        \"\"\"Summarize the PDF content\"\"\"\n",
    "        if not self.pdf_content:\n",
    "            return \"‚ùå No PDF loaded. Please load a PDF first.\"\n",
    "        \n",
    "        # Truncate content if too long\n",
    "        content = self.pdf_content[:10000] if len(self.pdf_content) > 10000 else self.pdf_content\n",
    "        \n",
    "        prompt = f\"Please provide a comprehensive summary of the following document content in about {max_length} characters:\\n\\n{content}\"\n",
    "        \n",
    "        try:\n",
    "            if self.config[\"model_provider\"] == \"mistral\":\n",
    "                messages = [UserMessage(content=prompt)]\n",
    "                return self._call_mistral(messages)\n",
    "            else:\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                return self._call_openai(messages)\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error generating summary: {e}\"\n",
    "    \n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"Ask a question about the PDF content\"\"\"\n",
    "        if not self.pdf_content:\n",
    "            return \"‚ùå No PDF loaded. Please load a PDF first.\"\n",
    "        \n",
    "        # Truncate content if too long\n",
    "        content = self.pdf_content[:8000] if len(self.pdf_content) > 8000 else self.pdf_content\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following document content, please answer the question.\n",
    "        \n",
    "Document content:\n",
    "{content}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer based on the document content. If the answer is not in the document, please say so.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if self.config[\"model_provider\"] == \"mistral\":\n",
    "                messages = [UserMessage(content=prompt)]\n",
    "                return self._call_mistral(messages)\n",
    "            else:\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                return self._call_openai(messages)\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error answering question: {e}\"\n",
    "    \n",
    "    def ask_question_stream(self, question: str):\n",
    "        \"\"\"Ask a question about the PDF content with streaming response\"\"\"\n",
    "        if not self.pdf_content:\n",
    "            yield \"‚ùå No PDF loaded. Please load a PDF first.\"\n",
    "            return\n",
    "        \n",
    "        # Truncate content if too long\n",
    "        content = self.pdf_content[:8000] if len(self.pdf_content) > 8000 else self.pdf_content\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following document content, please answer the question.\n",
    "        \n",
    "Document content:\n",
    "{content}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer based on the document content. If the answer is not in the document, please say so.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if self.config[\"model_provider\"] == \"mistral\":\n",
    "                messages = [UserMessage(content=prompt)]\n",
    "                for chunk in self._call_mistral_stream(messages):\n",
    "                    yield chunk\n",
    "            else:\n",
    "                # OpenAI streaming would go here\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                response = self._call_openai(messages)\n",
    "                yield response\n",
    "        except Exception as e:\n",
    "            yield f\"‚ùå Error answering question: {e}\"\n",
    "    \n",
    "    def get_document_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get information about the loaded document\"\"\"\n",
    "        if not self.pdf_content:\n",
    "            return {\"status\": \"No PDF loaded\"}\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"PDF loaded\",\n",
    "            \"pdf_path\": self.pdf_path,\n",
    "            \"text_length\": len(self.pdf_content),\n",
    "            \"word_count\": len(self.pdf_content.split()),\n",
    "            \"images_extracted\": len(self.extracted_images),\n",
    "            \"images\": [img[\"filename\"] for img in self.extracted_images]\n",
    "        }\n",
    "\n",
    "# Initialize the chat bot\n",
    "chatbot = PDFChatBot(CONFIG)\n",
    "print(\"üí¨ PDF Chat Bot initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Your PDF Document\n",
    "\n",
    "Change the `pdf_path` variable to point to your PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: transformer.pdf\n",
      "üîç Starting image extraction from: transformer.pdf\n",
      "üìÑ Total pages: 20\n",
      "üìÑ Page 1: Found 8 images\n",
      "  üñºÔ∏è  Image 1: 91x151, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img1.jpg\n",
      "  üñºÔ∏è  Image 2: 91x151, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img2.jpg\n",
      "  üñºÔ∏è  Image 3: 91x151, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img3.jpg\n",
      "  üñºÔ∏è  Image 4: 91x151, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img4.jpg\n",
      "  üñºÔ∏è  Image 5: 89x71, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img5.jpg\n",
      "  üñºÔ∏è  Image 6: 81x67, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img6.jpg\n",
      "  üñºÔ∏è  Image 7: 81x67, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img7.jpg\n",
      "  üñºÔ∏è  Image 8: 89x71, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page1_img8.jpg\n",
      "üìÑ Page 4: Found 1 images\n",
      "  üñºÔ∏è  Image 1: 494x62, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page4_img1.jpg\n",
      "üìÑ Page 15: Found 1 images\n",
      "  üñºÔ∏è  Image 1: 512x62, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page15_img1.jpg\n",
      "üìÑ Page 18: Found 2 images\n",
      "  üñºÔ∏è  Image 1: 312x118, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page18_img1.jpg\n",
      "  üñºÔ∏è  Image 2: 81x67, colorspace: ICCBased(RGB,sRGB IEC61966-2.1), channels: 3\n",
      "    ‚úÖ Saved: transformer_page18_img2.jpg\n",
      "\n",
      "‚úÖ Successfully extracted 12 images from transformer.pdf\n",
      "\n",
      "üìä Extraction Summary:\n",
      "  ‚Ä¢ transformer_page1_img1.jpg: 91x151, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page1_img2.jpg: 91x151, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page1_img3.jpg: 91x151, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page1_img4.jpg: 91x151, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page1_img5.jpg: 89x71, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page1_img6.jpg: 81x67, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page1_img7.jpg: 81x67, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page1_img8.jpg: 89x71, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page4_img1.jpg: 494x62, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page15_img1.jpg: 512x62, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page18_img1.jpg: 312x118, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "  ‚Ä¢ transformer_page18_img2.jpg: 81x67, ICCBased(RGB,sRGB IEC61966-2.1) [pixmap]\n",
      "‚úÖ PDF loaded successfully!\n",
      "üìÑ Text length: 66113 characters\n",
      "üñºÔ∏è Images extracted: 12\n",
      "\n",
      "üñºÔ∏è Extracted Images:\n",
      "  - transformer_page1_img1.jpg (Page 1, 91x151)\n",
      "  - transformer_page1_img2.jpg (Page 1, 91x151)\n",
      "  - transformer_page1_img3.jpg (Page 1, 91x151)\n",
      "  - transformer_page1_img4.jpg (Page 1, 91x151)\n",
      "  - transformer_page1_img5.jpg (Page 1, 89x71)\n",
      "  - transformer_page1_img6.jpg (Page 1, 81x67)\n",
      "  - transformer_page1_img7.jpg (Page 1, 81x67)\n",
      "  - transformer_page1_img8.jpg (Page 1, 89x71)\n",
      "  - transformer_page4_img1.jpg (Page 4, 494x62)\n",
      "  - transformer_page15_img1.jpg (Page 15, 512x62)\n",
      "  - transformer_page18_img1.jpg (Page 18, 312x118)\n",
      "  - transformer_page18_img2.jpg (Page 18, 81x67)\n"
     ]
    }
   ],
   "source": [
    "# Load your PDF file\n",
    "# This example uses transformer.pdf file, which stanford published document on transformer.\n",
    "# Get pdf file from https://web.stanford.edu/~jurafsky/slp3/9.pdf\n",
    "pdf_path = \"transformer.pdf\"  # Change this to your PDF file path\n",
    "\n",
    "# Example: pdf_path = \"./documents/research_paper.pdf\"\n",
    "# Example: pdf_path = \"./sample.pdf\"\n",
    "\n",
    "# Load the PDF (set extract_images=False if you don't want images)\n",
    "try:\n",
    "    result = chatbot.load_pdf(pdf_path, extract_images=True, debug=True)  # Set debug=True to see detailed extraction info\n",
    "    \n",
    "    # Show extracted images\n",
    "    if result[\"images\"]:\n",
    "        print(\"\\nüñºÔ∏è Extracted Images:\")\n",
    "        for img in result[\"images\"]:\n",
    "            print(f\"  - {img['filename']} (Page {img['page_number']}, {img['width']}x{img['height']})\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå PDF file not found: {pdf_path}\")\n",
    "    print(\"Please update the pdf_path variable with the correct path to your PDF file.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading PDF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Information:\n",
      "  status: PDF loaded\n",
      "  pdf_path: transformer.pdf\n",
      "  text_length: 66113\n",
      "  word_count: 7534\n",
      "  images_extracted: 12\n",
      "  images: ['transformer_page1_img1.jpg', 'transformer_page1_img2.jpg', 'transformer_page1_img3.jpg', 'transformer_page1_img4.jpg', 'transformer_page1_img5.jpg', 'transformer_page1_img6.jpg', 'transformer_page1_img7.jpg', 'transformer_page1_img8.jpg', 'transformer_page4_img1.jpg', 'transformer_page15_img1.jpg', 'transformer_page18_img1.jpg', 'transformer_page18_img2.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Get document information\n",
    "doc_info = chatbot.get_document_info()\n",
    "print(\"Document Information:\")\n",
    "for key, value in doc_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Summary\n",
    "\n",
    "Get an AI-generated summary of your PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generating summary...\n",
      "\n",
      "üìÑ Document Summary:\n",
      "==================================================\n",
      "The document discusses the transformer architecture, which has revolutionized speech and language processing. The transformer is a neural network designed for building large language models, particularly focusing on left-to-right language modeling. This involves predicting output tokens one by one based on prior context.\n",
      "\n",
      "### Key Components of the Transformer:\n",
      "1. **Self-Attention (Multi-Head Attention)**: This mechanism builds contextual representations of a token's meaning by integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans.\n",
      "2. **Transformer Blocks**: These are multi-layer networks consisting of a multi-head attention layer, feedforward networks, and layer normalization steps. They map input vectors to output vectors.\n",
      "3. **Input Encoding**: This component processes input tokens into contextual vector representations using an embedding matrix and a mechanism for encoding token position.\n",
      "4. **Language Modeling Head**: This takes the embedding output from the final transformer block, passes it through an unembedding matrix, and applies a softmax over the vocabulary to generate a single token for that column.\n",
      "\n",
      "### Detailed Mechanism:\n",
      "- **Attention**: Allows the model to build contextual representations by integrating information from other tokens. It computes a representation for a token at a particular layer by combining information from the previous layer with information about neighboring tokens.\n",
      "- **Self-Attention Distribution**: This involves weighing and combining representations from other tokens in the context to build the representation for tokens in the next layer.\n",
      "\n",
      "### Examples and Contextual Understanding:\n",
      "- **Contextual Meaning**: Words have richer meanings in context. For example, the word \"it\" in different sentences can refer to different entities based on the context.\n",
      "- **Agreement and Sense Disambiguation**: The transformer can handle grammatical agreement and sense disambiguation by integrating contextual information from distant words.\n",
      "\n",
      "### Formal Description of Attention:\n",
      "- **Simplified Attention**: At its core, attention is a weighted sum of context vectors. The weights (Œ±) are computed based on the similarity between the current token and prior tokens, using dot products and softmax normalization.\n",
      "\n",
      "### Future Chapters:\n",
      "- The document outlines that subsequent chapters will delve into the pretraining of language models, token generation via sampling, masked language modeling, prompting techniques, alignment with human preferences, and machine translation using the encoder-decoder architecture.\n",
      "\n",
      "In summary, the transformer architecture, with its self-attention mechanism, has become the standard for building large language models, enabling complex contextual understanding and significantly advancing the field of speech and language processing.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate summary\n",
    "print(\"üìù Generating summary...\")\n",
    "summary = chatbot.summarize(max_length=1500)\n",
    "print(\"\\nüìÑ Document Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(summary)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Questions About Your Document\n",
    "\n",
    "Now you can ask questions about your PDF document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: What is the main topic of this document?\n",
      "ü§ñ Answer:\n",
      "----------------------------------------\n",
      "The main topic of this document is the Transformer architecture, which is a neural network model widely used for building large language models. The document discusses the structure and components of the Transformer, including the mechanism called self-attention or multi-head attention, which helps the model learn how tokens relate to each other over large spans. The text also delves into the details of how the Transformer processes input tokens, uses attention to build contextual representations, and generates output tokens. Additionally, it mentions how the Transformer has significantly impacted the field of speech and language processing and will be a foundational concept in subsequent chapters of the textbook.\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Question: What are the key findings or conclusions?\n",
      "ü§ñ Answer:\n",
      "----------------------------------------\n",
      "‚ùå Error answering question: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Question: What are the key components of transformer?\n",
      "ü§ñ Answer:\n",
      "----------------------------------------\n",
      "‚ùå Error answering question: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ask questions about the document\n",
    "questions = [\n",
    "    \"What is the main topic of this document?\",\n",
    "    \"What are the key findings or conclusions?\",\n",
    "    \"What are the key components of transformer?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    print(\"üí¨ Answer:\")\n",
    "    print(\"-\" * 40)\n",
    "    answer = chatbot.ask_question(question)\n",
    "    print(answer)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat Interface\n",
    "\n",
    "Use this cell to ask custom questions about your document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Your Question: What is self attention? Is there multi-head attention?\n",
      "\n",
      "ü§ñ Answer:\n",
      "============================================================\n",
      "Based on the provided document content, here's a detailed answer to your questions:\n",
      "\n",
      "**What is self-attention?**\n",
      "\n",
      "Self-attention is a mechanism used in the transformer architecture that allows the model to weigh and combine representations from appropriate other tokens in the context from the previous layer (layer k-1) to build the representation for tokens in the current layer (layer k). It helps the model to understand the context and relations between words in a sentence, even when they are far apart. In essence, self-attention enables the model to focus on relevant tokens for a given task, such as generating the next word in a sentence.\n",
      "\n",
      "In the document, it is described as:\n",
      "\n",
      "\"Attention is the mechanism in the transformer that weighs and combines the representations from appropriate other tokens in the context from layer k‚àí1 to build the representation for tokens in layer k.\"\n",
      "\n",
      "**Is there multi-head attention?**\n",
      "\n",
      "Yes, the document mentions multi-head attention. It is briefly referenced in the description of a transformer block:\n",
      "\n",
      "\"Each block is a multi-layer network (a multi-head attention layer, feedforward networks and layer normalization steps) that maps an input vector x in column i (corresponding to input token i) to an output vector h.\"\n",
      "\n",
      "However, the document does not elaborate on the details of multi-head attention in the provided content. It only mentions that it will be introduced in the next sections, which are not included in the given text. So, while the document acknowledges the existence of multi-head attention, it does not provide a detailed explanation within the given content.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Interactive chat - change this question to ask about your document\n",
    "your_question = \"What is self attention? Is there multi-head attention?\"\n",
    "\n",
    "print(f\"‚ùì Your Question: {your_question}\")\n",
    "print(\"\\nüí¨ Answer:\")\n",
    "print(\"=\" * 60)\n",
    "answer = chatbot.ask_question(your_question)\n",
    "print(answer)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Your Question (Streaming): What is layer normalization?\n",
      "\n",
      "üí¨ Streaming Answer:\n",
      "============================================================\n",
      "‚ùå Error answering question: API error occurred: Status 429\n",
      "{\"object\":\"error\",\"message\":\"Service tier capacity exceeded for this model.\",\"type\":\"service_tier_capacity_exceeded\",\"param\":null,\"code\":\"3505\"}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Streaming chat example - works with Mistral\n",
    "your_streaming_question = \"What is layer normalization?\"\n",
    "\n",
    "if chatbot.config[\"model_provider\"] == \"mistral\":\n",
    "    print(f\"‚ùì Your Question (Streaming): {your_streaming_question}\")\n",
    "    print(\"\\nüí¨ Streaming Answer:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Collect the streamed response\n",
    "    full_response = \"\"\n",
    "    for chunk in chatbot.ask_question_stream(your_streaming_question):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        full_response += chunk\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "else:\n",
    "    print(\"Streaming is currently supported for Mistral only.\")\n",
    "    print(\"Switch to Mistral in the CONFIG section to use streaming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Chat Interface\n",
    "\n",
    "Use this cell to ask questions with streaming responses (real-time text generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch Between Models\n",
    "\n",
    "You can easily switch between Mistral and OpenAI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to OpenAI (if available)\n",
    "if OPENAI_AVAILABLE:\n",
    "    print(\"üîÑ Switching to OpenAI...\")\n",
    "    CONFIG[\"model_provider\"] = \"openai\"\n",
    "    chatbot_openai = PDFChatBot(CONFIG)\n",
    "    \n",
    "    # Load the same PDF\n",
    "    if chatbot.pdf_content:\n",
    "        chatbot_openai.pdf_content = chatbot.pdf_content\n",
    "        chatbot_openai.pdf_path = chatbot.pdf_path\n",
    "        chatbot_openai.extracted_images = chatbot.extracted_images\n",
    "        \n",
    "        # Ask the same question with OpenAI\n",
    "        question = \"What is this document about?\"\n",
    "        print(f\"\\n‚ùì Question (OpenAI): {question}\")\n",
    "        print(\"üí¨ Answer from OpenAI:\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = chatbot_openai.ask_question(question)\n",
    "        print(answer)\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No PDF loaded in the original chatbot.\")\n",
    "else:\n",
    "    print(\"OpenAI not available. Please install: pip install openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Additional helper functions for working with your PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Beginners\n",
    "\n",
    "1. **API Keys**: Make sure to set your API keys in the CONFIG section or as environment variables.\n",
    "\n",
    "2. **PDF Path**: Update the `pdf_path` variable to point to your actual PDF file.\n",
    "\n",
    "3. **Model Selection**: You can switch between Mistral and OpenAI by changing `CONFIG[\"model_provider\"]`.\n",
    "\n",
    "4. **Questions**: Modify the questions in the cells above to ask about your specific document.\n",
    "\n",
    "5. **Images**: If your PDF has images, they will be automatically extracted and saved as JPG files.\n",
    "\n",
    "6. **Streaming**: The streaming functionality allows real-time text generation (works with Mistral).\n",
    "\n",
    "7. **Self-Contained**: All PDF processing functions are now included in the notebook - no external files needed!\n",
    "\n",
    "8. **Error Handling**: If you encounter errors, make sure all required packages are installed.\n",
    "\n",
    "## What's New in This Version\n",
    "\n",
    "- **‚úÖ Self-Contained**: All PDF processing functions are now included in the notebook\n",
    "- **‚úÖ No External Dependencies**: No need for pdf_extractor folder or separate files\n",
    "- **‚úÖ Updated to new Mistral API**: Uses `Mistral` client with `chat.complete()` and `chat.stream()` methods\n",
    "- **‚úÖ Enhanced Documentation**: Each function has detailed documentation and examples\n",
    "- **‚úÖ Streaming Support**: Real-time text generation with proper response handling\n",
    "- **‚úÖ Improved Error Handling**: Better error messages and fallback options\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different types of questions\n",
    "- Experiment with both AI models\n",
    "- Test with different PDF documents\n",
    "- Try the streaming functionality\n",
    "- Modify the prompts to get better responses\n",
    "- Explore the individual PDF processing functions\n",
    "\n",
    "Happy chatting with your PDFs! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1,\n",
       "  'image_index': 1,\n",
       "  'filename': 'transformer_page1_img1.jpg',\n",
       "  'path': './transformer_page1_img1.jpg',\n",
       "  'width': 91,\n",
       "  'height': 151,\n",
       "  'size_bytes': 2782,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 1,\n",
       "  'image_index': 2,\n",
       "  'filename': 'transformer_page1_img2.jpg',\n",
       "  'path': './transformer_page1_img2.jpg',\n",
       "  'width': 91,\n",
       "  'height': 151,\n",
       "  'size_bytes': 2782,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 1,\n",
       "  'image_index': 3,\n",
       "  'filename': 'transformer_page1_img3.jpg',\n",
       "  'path': './transformer_page1_img3.jpg',\n",
       "  'width': 91,\n",
       "  'height': 151,\n",
       "  'size_bytes': 2782,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 1,\n",
       "  'image_index': 4,\n",
       "  'filename': 'transformer_page1_img4.jpg',\n",
       "  'path': './transformer_page1_img4.jpg',\n",
       "  'width': 91,\n",
       "  'height': 151,\n",
       "  'size_bytes': 2782,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 1,\n",
       "  'image_index': 5,\n",
       "  'filename': 'transformer_page1_img5.jpg',\n",
       "  'path': './transformer_page1_img5.jpg',\n",
       "  'width': 89,\n",
       "  'height': 71,\n",
       "  'size_bytes': 2760,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 1,\n",
       "  'image_index': 6,\n",
       "  'filename': 'transformer_page1_img6.jpg',\n",
       "  'path': './transformer_page1_img6.jpg',\n",
       "  'width': 81,\n",
       "  'height': 67,\n",
       "  'size_bytes': 2758,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 1,\n",
       "  'image_index': 7,\n",
       "  'filename': 'transformer_page1_img7.jpg',\n",
       "  'path': './transformer_page1_img7.jpg',\n",
       "  'width': 81,\n",
       "  'height': 67,\n",
       "  'size_bytes': 2758,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 1,\n",
       "  'image_index': 8,\n",
       "  'filename': 'transformer_page1_img8.jpg',\n",
       "  'path': './transformer_page1_img8.jpg',\n",
       "  'width': 89,\n",
       "  'height': 71,\n",
       "  'size_bytes': 2760,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 4,\n",
       "  'image_index': 1,\n",
       "  'filename': 'transformer_page4_img1.jpg',\n",
       "  'path': './transformer_page4_img1.jpg',\n",
       "  'width': 494,\n",
       "  'height': 62,\n",
       "  'size_bytes': 2831,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 15,\n",
       "  'image_index': 1,\n",
       "  'filename': 'transformer_page15_img1.jpg',\n",
       "  'path': './transformer_page15_img1.jpg',\n",
       "  'width': 512,\n",
       "  'height': 62,\n",
       "  'size_bytes': 2834,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 18,\n",
       "  'image_index': 1,\n",
       "  'filename': 'transformer_page18_img1.jpg',\n",
       "  'path': './transformer_page18_img1.jpg',\n",
       "  'width': 312,\n",
       "  'height': 118,\n",
       "  'size_bytes': 2849,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'},\n",
       " {'page_number': 18,\n",
       "  'image_index': 2,\n",
       "  'filename': 'transformer_page18_img2.jpg',\n",
       "  'path': './transformer_page18_img2.jpg',\n",
       "  'width': 81,\n",
       "  'height': 67,\n",
       "  'size_bytes': 2758,\n",
       "  'colorspace': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
       "  'converted': False,\n",
       "  'extraction_method': 'pixmap'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.extracted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Found 12 images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display extracted images (if any)\n",
    "if chatbot.extracted_images:\n",
    "    print(f\"üñºÔ∏è Found {len(chatbot.extracted_images)} images:\")\n",
    "    \n",
    "    # Try to display images using matplotlib\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "        \n",
    "        fig, axes = plt.subplots(1, min(3, len(chatbot.extracted_images)), figsize=(15, 5))\n",
    "        if len(chatbot.extracted_images) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, img_info in enumerate(chatbot.extracted_images[:3]):\n",
    "            if os.path.exists(img_info['path']):\n",
    "                img = mpimg.imread(img_info['path'])\n",
    "                axes[i].imshow(img)\n",
    "                axes[i].set_title(f\"Page {img_info['page_number']}\")\n",
    "                axes[i].axis('off')\n",
    "            else:\n",
    "                axes[i].text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Install matplotlib to display images: pip install matplotlib\")\n",
    "        for img in chatbot.extracted_images:\n",
    "            print(f\"  - {img['filename']} saved to {img['path']}\")\n",
    "else:\n",
    "    print(\"No images extracted from the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Beginners\n",
    "\n",
    "1. **API Keys**: Make sure to set your API keys in the CONFIG section or as environment variables.\n",
    "\n",
    "2. **PDF Path**: Update the `pdf_path` variable to point to your actual PDF file.\n",
    "\n",
    "3. **Model Selection**: You can switch between Mistral and OpenAI by changing `CONFIG[\"model_provider\"]`.\n",
    "\n",
    "4. **Questions**: Modify the questions in the cells above to ask about your specific document.\n",
    "\n",
    "5. **Images**: If your PDF has images, they will be automatically extracted and saved as JPG files.\n",
    "\n",
    "6. **Error Handling**: If you encounter errors, make sure all required packages are installed.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different types of questions\n",
    "- Experiment with both AI models\n",
    "- Test with different PDF documents\n",
    "- Modify the prompts to get better responses\n",
    "\n",
    "Happy chatting with your PDFs! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
